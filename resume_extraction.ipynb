{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 0: Load the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "from pypdf import PdfReader\n",
    "import boto3\n",
    "from langchain_aws import BedrockLLM\n",
    "from langchain_community.chat_models import BedrockChat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Resume \n",
    "RESUME_FILE_PATH = \"resume_extraction/data/test_resume.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Ng Jia Yin Email: ng.jiayin@outlook.com | Mobile: 90501881 linkedin.com/in/jia-yin-ng/ | github.com/ngjiayin  EDUCATION National University of Singapore (NUS) Bachelor of Science (Honours) in Data Science and Analytics Aug 2021 – Present • Expected Date of Graduation: Dec 2025 • Relevant Coursework: Data Visualisation, Decision Trees for Machine Learning, Artificial Intelligence: Technology and Impact, Data Structures and Algorithms, Mathematical Statistics, Multivariable Calculus  WORK EXPERIENCE Johnson & Johnson, Analytics Intern Jan 2024 – May 2024 • Managed end-to-end financial forecasting for the Long Range Financial Plan (LRFP) spanning from 2024 to 2028, managing sales figures totalling more than $4 billion. • Implemented data validation and cleaning processes using Microsoft Excel, resulting in a 15% reduction in data errors. • Utilised design thinking methodologies to develop a 90-second brand launch video for an in-house data analytics platform, enhancing brand engagement and visibility.  KPMG, Tax Processor Intern Jan 2021 – Jun 2021 • Prepared and filed over 100 personal income tax returns using Microsoft Excel and internal tax software over the span of 5 months. • Communicated effectively with clients and team members to identify and obtain necessary financial records in compliance with IRAS regulations.  PROJECTS Lego Data Visualisation Nov 2023 • Utilised dplyr and ggplot2 packages to generate 3 visualisations (line plot, bar graph, scatter plot) to analyse the evolution of Lego set colours from 1949 to 2022. • Pre-processed and cleaned 5 csv files and presented the findings and discussion in a Rmd file.  Stock Price Prediction Oct 2022 • Implemented various regression methods in Python to predict stock prices, achieving a minimal mean-squared error (MSE) of 3.07%. • Conducted feature engineering and employed Principal Component Analysis (PCA) for exploratory analysis.  TECHNICAL SKILLS  • Programming and Database Languages: Python, R, SQL and Java • Python Libraries: NumPy, pandas (data manipulation), Matplotlib, seaborn (data visualization), scikit-learn, TensorFlow, PyTorch (machine learning), OpenCV (image processing) • R Libraries: tidyverse, dplyr, ggplot2 • Other Tools: Jupyter Notebooks, Tableau, Microsoft Excel  CO-CURRICULAR ACTIVITIES Workshop Executive, NUS Statistics and Data Science Society Jul 2023 – Present • Collaborate with team members to create engaging presentation slides and Python code for Data Cleaning, Statistical Testing, Image Processing and Machine Learning workshops. • Deliver insightful data science content to more than 150 NUS students, effectively conveying complex concepts and fostering a conducive learning environment.  '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert PDF Resume to Text\n",
    "\n",
    "resume_reader = PdfReader(RESUME_FILE_PATH)\n",
    "resume_text = \"\"\n",
    "\n",
    "for page in resume_reader.pages:\n",
    "    resume_text += page.extract_text()\n",
    "\n",
    "\n",
    "resume_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: Intialise AWS Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise bedrock client\n",
    "\n",
    "llm = BedrockLLM(\n",
    "    model_id=\"mistral.mixtral-8x7b-instruct-v0:1\",\n",
    "    model_kwargs={\"temperature\": 0.3, \"max_tokens\":4076, \"top_p\":0.1,\"top_k\":50}\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I hope you’re having a great week so far. I’m excited to share a new recipe with you today! I’ve been making this recipe for years and it’s one of my favorites. It’s a simple, healthy, and delicious recipe that’s perfect for breakfast, lunch, or dinner.\n",
      "\n",
      "This recipe is for a quinoa and black bean salad. It’s made with cooked quinoa, black beans, corn, cherry tomatoes, red onion, avocado, lime juice, olive oil, salt, and pepper. It’s a hearty and filling salad that’s packed with protein, fiber, and healthy fats.\n",
      "\n",
      "To make the salad, start by cooking the quinoa according to the package instructions. Once the quinoa is cooked, let it cool to room temperature.\n",
      "\n",
      "Next, drain and rinse the black beans and corn. Cut the cherry tomatoes in half and dice the red onion.\n",
      "\n",
      "In a large bowl, combine the cooked quinoa, black beans, corn, cherry tomatoes, red onion, and avocado.\n",
      "\n",
      "In a small bowl, whisk together the lime juice, olive oil, salt, and pepper. Pour the dressing over the salad and toss to combine.\n",
      "\n",
      "That’s it! This salad is best served chilled, so I recommend making it ahead of time and letting it sit in the fridge for at least an hour before serving. It will keep in the fridge for up to 3 days.\n",
      "\n",
      "I hope you enjoy this recipe! It’s one of my go-to’s when I’m looking for a healthy and delicious meal. Let me know if you give it a try!\n",
      "\n",
      "Quinoa and Black Bean Salad\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "* 1 cup quinoa, cooked according to package instructions\n",
      "* 1 (15-ounce) can black beans, drained and rinsed\n",
      "* 1 cup corn, drained and rinsed\n",
      "* 1 cup cherry tomatoes, halved\n",
      "* 1/2 red onion, diced\n",
      "* 1 avocado, diced\n",
      "* 2 limes, juiced\n",
      "* 2 tablespoons olive oil\n",
      "* Salt and pepper, to taste\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. In a large bowl, combine the cooked quinoa, black beans, corn, cherry tomatoes, red onion, and avocado.\n",
      "2. In a small bowl, whisk together the lime juice, olive oil, salt, and pepper.\n",
      "3. Pour the dressing over the salad and toss to combine.\n",
      "4. Serve chilled. Enjoy!\n",
      "\n",
      "Notes:\n",
      "\n",
      "* This salad is best made ahead of time and chilled in the fridge for at least an hour before serving.\n",
      "* It will keep in the fridge for up to 3 days.\n",
      "* Feel free to add any additional vegetables or protein to this salad to make it your own!\n"
     ]
    }
   ],
   "source": [
    "# Invoke the llm\n",
    "response = llm.invoke(\"Hello! How are you today?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3: Prompt Design & Fine-Tuning & Data Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the skills from the resume\n",
    "# Competencies are divided into: Core, Functional, Technical, and Leadership Skills\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hard_skill_entity_extraction_prompt = \"\"\"\n",
    "You are a career counselor. Your task is to extract skill entities from the given text, which can be a resume or a job description.\n",
    "\n",
    "Skill Entities:\n",
    "Hard Skills: Also called technical skills, these are job-specific and relevant to each position and seniority level. In other words, each position in every company will require unique hard skills.\n",
    "\n",
    "TASK:\n",
    "1. Perform a Part-of-Speech (POS) Tagging on the text.\n",
    "2. Using the POS-tagged resume, perform Name Entity Recognition to identify all explicitly stated skill entities.\n",
    "3. For each skill, provide an explanation of skill demonstration using context from the text and the context's contribution to the skill.\n",
    "4. State the context as it appears in the resume, without extrapolating it with other experiences.\n",
    "\n",
    "----------------------\n",
    "Format Instruction:\n",
    "{format_instructions}\n",
    "----------------------\n",
    "\n",
    "----------------------\n",
    "Text: {text}\n",
    "----------------------\n",
    "\"\"\"\n",
    "\n",
    "soft_skill_entity_extraction_prompt = \"\"\"\n",
    "You are a career counselor. \n",
    "Your task is to extract skill entities from the given text, text can be a resume or a Job Description.\n",
    "\n",
    " ------------------------------- \n",
    " Skill Entities: Soft Skills: the term ‘soft skills’ refers to a broad set of skills, behaviors, attitudes and personal qualities that allow people to adapt effectively to their environment, to work well with others, to perform well, and to achieve their goals.\n",
    "------------------------------- \n",
    "\n",
    "TASK: \n",
    "1. Perform a Part-of-Speech(POS) Tagging on the text. \n",
    "2. Using the POS-tagged resume, perform Name Entity Recognition to identify explicit skills. \n",
    "3. Do not assume skills, only extract skills given in the text. \n",
    "4. For each skill provide a justification of skill demonstration in resume. For Skill: Python Example: \"User Demonstrated Python skill by developing a web application using Django framework.\"\n",
    "\n",
    "Only return the JSON. \n",
    "------------------------------- \n",
    "Format Instruction: \n",
    "{format_instructions}\n",
    "------------------------------- \n",
    "\n",
    "------------------------------- \n",
    "text: {text}\n",
    "-------------------------------\n",
    "\"\"\"\n",
    "\n",
    "modify_skills_entity_extraction_prompt = \"\"\"\n",
    "\n",
    "TASK: \n",
    "1. Understand User Modified Skill Name,Modification justification and the modification action user wants to perform.\n",
    "2. For add or modify Action , check if the skill_name already exists, if it does use the explanation to add to the current skill description.\n",
    "2. For delete action, use the explanation to determine if the whole skills needs to be deleted or just parts of the current justification.\n",
    "\n",
    "Only return the affected skill_name and skill_justification. \n",
    "If the whole skill is deleted , return an empty justification.\n",
    "If the user input justification is not enough or enough infromation about the action is not provided, simply return an empty skill_name.\n",
    "\n",
    "Only return the JSON. \n",
    "------------------------------- \n",
    "Format Instruction: \n",
    "{format_instructions}\n",
    "------------------------------- \n",
    "\n",
    "------------------------------- \n",
    "user input skill: {skill_name}\n",
    "user justification: {user_justification}\n",
    "action: {action}\n",
    "All Skills: {all_skills}\n",
    "-------------------------------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "fix_format_instruction = \"\"\"\n",
    "--------------\n",
    "{instructions}\n",
    "--------------\n",
    "Completion:\n",
    "--------------\n",
    "{completion}\n",
    "--------------\n",
    "Above, the Completion did not satisfy the constraints given in the Instructions.\n",
    "Error:\n",
    "--------------\n",
    "{error}\n",
    "--------------\n",
    "Please try again. Please only respond with an answer that satisfies the constraints laid out in the Instructions.\n",
    "Important:  Only correct the structural issues within the JSON format. Do not modify the existing data values themselves:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Competency(BaseModel):\n",
    "    competency_type: str = Field(..., description=\"Type of competency:Functional, Technical\")\n",
    "    competency_name: str = Field(..., description=\"Name of the competency\")\n",
    "    competency_description: str = Field(..., description=\"Description of how the client has demonstrated the competency in their resume\")\n",
    "\n",
    "class Comptencies(BaseModel):\n",
    "    competencies: List[Competency] = Field(..., description=\"List of competencies extracted from the resume\")\n",
    "\n",
    "class Skill(BaseModel):\n",
    "    skill: str = Field(..., description=\"Name of the skill\")\n",
    "    # skill_level_justification : str = Field(..., description=\"Justification of the skill level\")    \n",
    "    skill_level: str = Field(..., description=\"Level of the skill\", enum=[\"Beginner\", \"Intermediate\", \"Expert\"])\n",
    "    # skill_justification: str = Field(..., description=\"Justification of the extracted skill and level\")# \n",
    "\n",
    "class Skills(BaseModel):\n",
    "    skills: List[Skill] = Field(..., description=\"List of skills and their corresponding skill level extracted from the resume\")\n",
    "    \n",
    "class SoftSkill(BaseModel):\n",
    "    skill: str = Field(..., description=\"Name of the skill\")\n",
    "    skill_justification: str = Field(..., description=\"explanation of skill demonstration using context from the text and the context's contribution to the skill\")#\n",
    "\n",
    "class SoftSkills(BaseModel):\n",
    "    soft_skills: List[SoftSkill] = Field(..., description=\"List of soft skills \")\n",
    "\n",
    "class HardSkill(BaseModel):\n",
    "    skill: str = Field(..., description=\"Name of the skill\")\n",
    "    skill_explanation: str = Field(..., description=\"explanation of skill demonstration using context from the text and the context's contribution to the skill\")#\n",
    "\n",
    "class HardSkills(BaseModel):\n",
    "    hard_skills: List[HardSkill] = Field(..., description=\"List of hard skills \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 4: Extract Comptencies from the Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser, OutputFixingParser\n",
    "from langchain.chains import TransformChain\n",
    "\n",
    "def fix_chain_fun(inputs):    \n",
    "    fix_prompt = PromptTemplate.from_template(fix_format_instruction)\n",
    "    fix_prompt_str = fix_prompt.invoke({'instructions':inputs['instructions'],\n",
    "                                        'completion':inputs['completion'],\n",
    "                                        'error':inputs['error']}).text\n",
    "    \n",
    "    #print(fix_prompt_str)\n",
    "    \n",
    "    completion = llm.invoke(fix_prompt_str)\n",
    "\n",
    "    # return {\"completion\": completion}\n",
    "    \n",
    "    return {\"completion\": completion}\n",
    "\n",
    "fix_chain = TransformChain(\n",
    "    input_variables = [\"instructions\", \"completion\", \"error\"],output_variables=[\"completion\"], transform=fix_chain_fun\n",
    ")\n",
    "\n",
    "\n",
    "def get_hard_skills(resume_text: str) ->  HardSkills:\n",
    "    '''Takes in User's Resume Text, extracts hard skills form resume , return a JSON of all extracted skills and their explanations using context from Resume.\n",
    "    \n",
    "    Args:\n",
    "        resume_text: Resume String for skill extraction.\n",
    "    '''\n",
    "    # Invoke the LLM\n",
    "    parser = PydanticOutputParser(pydantic_object= HardSkills)\n",
    "    \n",
    "    fix_parser = OutputFixingParser(\n",
    "        parser=parser,\n",
    "        retry_chain=fix_chain,\n",
    "        max_retries=2\n",
    "    )\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template = hard_skill_entity_extraction_prompt, \n",
    "        input_variables=[\"text\"],\n",
    "        partial_variables= {\"format_instructions\": parser.get_format_instructions()})\n",
    "    \n",
    "    prompt_str = prompt.format(text=resume_text)\n",
    "\n",
    "    print(prompt_str)\n",
    "    \n",
    "    response = llm.invoke(prompt_str)\n",
    "\n",
    "\n",
    "    print(f\"Response is : {response}\")\n",
    "\n",
    "    fixed_response = fix_parser.invoke(response).dict()\n",
    "\n",
    "    return fixed_response\n",
    "\n",
    "def get_soft_skills(resume_text: str) ->  SoftSkills:\n",
    "    # Invoke the LLM\n",
    "    parser = PydanticOutputParser(pydantic_object= SoftSkills)\n",
    "    \n",
    "    fix_parser = OutputFixingParser(\n",
    "        parser=parser,\n",
    "        retry_chain=fix_chain,\n",
    "        max_retries=2\n",
    "    )\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template = soft_skill_entity_extraction_prompt, \n",
    "        input_variables=[\"text\"],\n",
    "        partial_variables= {\"format_instructions\": parser.get_format_instructions()})\n",
    "    \n",
    "    prompt_str = prompt.format(text=resume_text)\n",
    "\n",
    "    print(prompt_str)\n",
    "    \n",
    "    response = llm.invoke(prompt_str)\n",
    "\n",
    "\n",
    "    print(f\"Response is : {response}\")\n",
    "\n",
    "    fixed_response = fix_parser.invoke(response).dict()\n",
    "\n",
    "    return fixed_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a career counselor. Your task is to extract skill entities from the given text, which can be a resume or a job description.\n",
      "\n",
      "Skill Entities:\n",
      "Hard Skills: Also called technical skills, these are job-specific and relevant to each position and seniority level. In other words, each position in every company will require unique hard skills.\n",
      "\n",
      "TASK:\n",
      "1. Perform a Part-of-Speech (POS) Tagging on the text.\n",
      "2. Using the POS-tagged resume, perform Name Entity Recognition to identify all explicitly stated skill entities.\n",
      "3. For each skill, provide an explanation of skill demonstration using context from the text and the context's contribution to the skill.\n",
      "4. State the context as it appears in the resume, without extrapolating it with other experiences.\n",
      "\n",
      "----------------------\n",
      "Format Instruction:\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"$defs\": {\"HardSkill\": {\"properties\": {\"skill\": {\"description\": \"Name of the skill\", \"title\": \"Skill\", \"type\": \"string\"}, \"skill_explanation\": {\"description\": \"explanation of skill demonstration using context from the text and the context's contribution to the skill\", \"title\": \"Skill Explanation\", \"type\": \"string\"}}, \"required\": [\"skill\", \"skill_explanation\"], \"title\": \"HardSkill\", \"type\": \"object\"}}, \"properties\": {\"hard_skills\": {\"description\": \"List of hard skills \", \"items\": {\"$ref\": \"#/$defs/HardSkill\"}, \"title\": \"Hard Skills\", \"type\": \"array\"}}, \"required\": [\"hard_skills\"]}\n",
      "```\n",
      "----------------------\n",
      "\n",
      "----------------------\n",
      "Text:  Ng Jia Yin Email: ng.jiayin@outlook.com | Mobile: 90501881 linkedin.com/in/jia-yin-ng/ | github.com/ngjiayin  EDUCATION National University of Singapore (NUS) Bachelor of Science (Honours) in Data Science and Analytics Aug 2021 – Present • Expected Date of Graduation: Dec 2025 • Relevant Coursework: Data Visualisation, Decision Trees for Machine Learning, Artificial Intelligence: Technology and Impact, Data Structures and Algorithms, Mathematical Statistics, Multivariable Calculus  WORK EXPERIENCE Johnson & Johnson, Analytics Intern Jan 2024 – May 2024 • Managed end-to-end financial forecasting for the Long Range Financial Plan (LRFP) spanning from 2024 to 2028, managing sales figures totalling more than $4 billion. • Implemented data validation and cleaning processes using Microsoft Excel, resulting in a 15% reduction in data errors. • Utilised design thinking methodologies to develop a 90-second brand launch video for an in-house data analytics platform, enhancing brand engagement and visibility.  KPMG, Tax Processor Intern Jan 2021 – Jun 2021 • Prepared and filed over 100 personal income tax returns using Microsoft Excel and internal tax software over the span of 5 months. • Communicated effectively with clients and team members to identify and obtain necessary financial records in compliance with IRAS regulations.  PROJECTS Lego Data Visualisation Nov 2023 • Utilised dplyr and ggplot2 packages to generate 3 visualisations (line plot, bar graph, scatter plot) to analyse the evolution of Lego set colours from 1949 to 2022. • Pre-processed and cleaned 5 csv files and presented the findings and discussion in a Rmd file.  Stock Price Prediction Oct 2022 • Implemented various regression methods in Python to predict stock prices, achieving a minimal mean-squared error (MSE) of 3.07%. • Conducted feature engineering and employed Principal Component Analysis (PCA) for exploratory analysis.  TECHNICAL SKILLS  • Programming and Database Languages: Python, R, SQL and Java • Python Libraries: NumPy, pandas (data manipulation), Matplotlib, seaborn (data visualization), scikit-learn, TensorFlow, PyTorch (machine learning), OpenCV (image processing) • R Libraries: tidyverse, dplyr, ggplot2 • Other Tools: Jupyter Notebooks, Tableau, Microsoft Excel  CO-CURRICULAR ACTIVITIES Workshop Executive, NUS Statistics and Data Science Society Jul 2023 – Present • Collaborate with team members to create engaging presentation slides and Python code for Data Cleaning, Statistical Testing, Image Processing and Machine Learning workshops. • Deliver insightful data science content to more than 150 NUS students, effectively conveying complex concepts and fostering a conducive learning environment.  \n",
      "----------------------\n",
      "\n",
      "Response is : \n",
      "```json\n",
      "{\n",
      "  \"hard_skills\": [\n",
      "    {\n",
      "      \"skill\": \"Financial forecasting\",\n",
      "      \"skill_explanation\": \"Managed end-to-end financial forecasting for the Long Range Financial Plan (LRFP) spanning from 2024 to 2028, managing sales figures totalling more than $4 billion.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"Data validation and cleaning\",\n",
      "      \"skill_explanation\": \"Implemented data validation and cleaning processes using Microsoft Excel, resulting in a 15% reduction in data errors.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"Design thinking methodologies\",\n",
      "      \"skill_explanation\": \"Utilised design thinking methodologies to develop a 90-second brand launch video for an in-house data analytics platform, enhancing brand engagement and visibility.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"Personal income tax returns preparation\",\n",
      "      \"skill_explanation\": \"Prepared and filed over 100 personal income tax returns using Microsoft Excel and internal tax software over the span of 5 months.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"Data visualization\",\n",
      "      \"skill_explanation\": \"Utilised dplyr and ggplot2 packages to generate 3 visualisations (line plot, bar graph, scatter plot) to analyse the evolution of Lego set colours from 1949 to 2022.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"Stock price prediction\",\n",
      "      \"skill_explanation\": \"Implemented various regression methods in Python to predict stock prices, achieving a minimal mean-squared error (MSE) of 3.07%.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"Programming and Database Languages\",\n",
      "      \"skill_explanation\": \"Proficient in Python, R, SQL and Java.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"Python Libraries\",\n",
      "      \"skill_explanation\": \"Proficient in NumPy, pandas (data manipulation), Matplotlib, seaborn (data visualization), scikit-learn, TensorFlow, PyTorch (machine learning), OpenCV (image processing).\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"R Libraries\",\n",
      "      \"skill_explanation\": \"Proficient in tidyverse, dplyr, ggplot2.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"Other Tools\",\n",
      "      \"skill_explanation\": \"Proficient in Jupyter Notebooks, Tableau, Microsoft Excel.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "You are a career counselor. \n",
      "Your task is to extract skill entities from the given text, text can be a resume or a Job Description.\n",
      "\n",
      " ------------------------------- \n",
      " Skill Entities: Soft Skills: the term ‘soft skills’ refers to a broad set of skills, behaviors, attitudes and personal qualities that allow people to adapt effectively to their environment, to work well with others, to perform well, and to achieve their goals.\n",
      "------------------------------- \n",
      "\n",
      "TASK: \n",
      "1. Perform a Part-of-Speech(POS) Tagging on the text. \n",
      "2. Using the POS-tagged resume, perform Name Entity Recognition to identify explicit skills. \n",
      "3. Do not assume skills, only extract skills given in the text. \n",
      "4. For each skills provide an explanation of skill demonstration in resume and it's contribution to the skill. \n",
      "\n",
      "Only return the JSON. \n",
      "------------------------------- \n",
      "Format Instruction: \n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"$defs\": {\"SoftSkill\": {\"properties\": {\"skill\": {\"description\": \"Name of the skill\", \"title\": \"Skill\", \"type\": \"string\"}, \"skill_justification\": {\"description\": \"explanation of skill demonstration using context from the text and the context's contribution to the skill\", \"title\": \"Skill Justification\", \"type\": \"string\"}}, \"required\": [\"skill\", \"skill_justification\"], \"title\": \"SoftSkill\", \"type\": \"object\"}}, \"properties\": {\"soft_skills\": {\"description\": \"List of soft skills \", \"items\": {\"$ref\": \"#/$defs/SoftSkill\"}, \"title\": \"Soft Skills\", \"type\": \"array\"}}, \"required\": [\"soft_skills\"]}\n",
      "```\n",
      "------------------------------- \n",
      "\n",
      "------------------------------- \n",
      "text:  Ng Jia Yin Email: ng.jiayin@outlook.com | Mobile: 90501881 linkedin.com/in/jia-yin-ng/ | github.com/ngjiayin  EDUCATION National University of Singapore (NUS) Bachelor of Science (Honours) in Data Science and Analytics Aug 2021 – Present • Expected Date of Graduation: Dec 2025 • Relevant Coursework: Data Visualisation, Decision Trees for Machine Learning, Artificial Intelligence: Technology and Impact, Data Structures and Algorithms, Mathematical Statistics, Multivariable Calculus  WORK EXPERIENCE Johnson & Johnson, Analytics Intern Jan 2024 – May 2024 • Managed end-to-end financial forecasting for the Long Range Financial Plan (LRFP) spanning from 2024 to 2028, managing sales figures totalling more than $4 billion. • Implemented data validation and cleaning processes using Microsoft Excel, resulting in a 15% reduction in data errors. • Utilised design thinking methodologies to develop a 90-second brand launch video for an in-house data analytics platform, enhancing brand engagement and visibility.  KPMG, Tax Processor Intern Jan 2021 – Jun 2021 • Prepared and filed over 100 personal income tax returns using Microsoft Excel and internal tax software over the span of 5 months. • Communicated effectively with clients and team members to identify and obtain necessary financial records in compliance with IRAS regulations.  PROJECTS Lego Data Visualisation Nov 2023 • Utilised dplyr and ggplot2 packages to generate 3 visualisations (line plot, bar graph, scatter plot) to analyse the evolution of Lego set colours from 1949 to 2022. • Pre-processed and cleaned 5 csv files and presented the findings and discussion in a Rmd file.  Stock Price Prediction Oct 2022 • Implemented various regression methods in Python to predict stock prices, achieving a minimal mean-squared error (MSE) of 3.07%. • Conducted feature engineering and employed Principal Component Analysis (PCA) for exploratory analysis.  TECHNICAL SKILLS  • Programming and Database Languages: Python, R, SQL and Java • Python Libraries: NumPy, pandas (data manipulation), Matplotlib, seaborn (data visualization), scikit-learn, TensorFlow, PyTorch (machine learning), OpenCV (image processing) • R Libraries: tidyverse, dplyr, ggplot2 • Other Tools: Jupyter Notebooks, Tableau, Microsoft Excel  CO-CURRICULAR ACTIVITIES Workshop Executive, NUS Statistics and Data Science Society Jul 2023 – Present • Collaborate with team members to create engaging presentation slides and Python code for Data Cleaning, Statistical Testing, Image Processing and Machine Learning workshops. • Deliver insightful data science content to more than 150 NUS students, effectively conveying complex concepts and fostering a conducive learning environment.  \n",
      "-------------------------------\n",
      "\n",
      "Response is : \n",
      "```json\n",
      "{\n",
      "  \"soft_skills\": [\n",
      "    {\n",
      "      \"skill\": \"Collaboration\",\n",
      "      \"skill_justification\": \"As a workshop executive for the NUS Statistics and Data Science Society, Ng Jia Yin collaborates with team members to create engaging presentation slides and Python code for Data Cleaning, Statistical Testing, Image Processing and Machine Learning workshops. This demonstrates Ng Jia Yin's ability to work effectively with others and contribute to a shared goal.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"Communication\",\n",
      "      \"skill_justification\": \"As a workshop executive for the NUS Statistics and Data Science Society, Ng Jia Yin delivers insightful data science content to more than 150 NUS students, effectively conveying complex concepts and fostering a conducive learning environment. This demonstrates Ng Jia Yin's ability to communicate effectively and clearly.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"Data Analysis\",\n",
      "      \"skill_justification\": \"Ng Jia Yin implemented various regression methods in Python to predict stock prices, achieving a minimal mean-squared error (MSE) of 3.07%. This demonstrates Ng Jia Yin's ability to analyze and interpret data to make informed decisions.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"Data Cleaning\",\n",
      "      \"skill_justification\": \"Ng Jia Yin managed end-to-end financial forecasting for the Long Range Financial Plan (LRFP) spanning from 2024 to 2028, managing sales figures totalling more than $4 billion. Ng Jia Yin also prepared and filed over 100 personal income tax returns using Microsoft Excel and internal tax software over the span of 5 months. These experiences demonstrate Ng Jia Yin's ability to clean and preprocess data for further analysis.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"Data Visualization\",\n",
      "      \"skill_justification\": \"Ng Jia Yin utilized design thinking methodologies to develop a 90-second brand launch video for an in-house data analytics platform, enhancing brand engagement and visibility. Ng Jia Yin also utilized the dplyr and ggplot2 packages to generate 3 visualisations (line plot, bar graph, scatter plot) to analyse the evolution of Lego set colours from 1949 to 2022. These experiences demonstrate Ng Jia Yin's ability to effectively visualize data to communicate insights and findings.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"Machine Learning\",\n",
      "      \"skill_justification\": \"Ng Jia Yin implemented various regression methods in Python to predict stock prices, achieving a minimal mean-squared error (MSE) of 3.07%. Ng Jia Yin also utilized the scikit-learn and TensorFlow libraries for machine learning tasks. These experiences demonstrate Ng Jia Yin's ability to apply machine learning techniques to solve complex problems.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"Project Management\",\n",
      "      \"skill_justification\": \"Ng Jia Yin managed end-to-end financial forecasting for the Long Range Financial Plan (LRFP) spanning from 2024 to 2028, managing sales figures totalling more than $4 billion. Ng Jia Yin also prepared and filed over 100 personal income tax returns using Microsoft Excel and internal tax software over the span of 5 months. These experiences demonstrate Ng Jia Yin's ability to manage projects effectively and efficiently.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"Time Management\",\n",
      "      \"skill_justification\": \"Ng Jia Yin managed end-to-end financial forecasting for the Long Range Financial Plan (LRFP) spanning from 2024 to 2028, managing sales figures totalling more than $4 billion. Ng Jia Yin also prepared and filed over 100 personal income tax returns using Microsoft Excel and internal tax software over the span of 5 months. These experiences demonstrate Ng Jia Yin's ability to manage time effectively and efficiently.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# extracted_skills = extract_skills(resume_text)\n",
    "extract_hard_skills = extract_hard_skills(resume_text)\n",
    "\n",
    "extract_soft_skills = extract_soft_skills(resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the extracted competencies to a JSON file\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "with open(f\"candidate_soft_extracted.json\", \"w\") as f:\n",
    "    json.dump(extract_soft_skills, f, indent=4) \n",
    "\n",
    "with open(f\"candidate_hard_extracted.json\", \"w\") as f:\n",
    "    json.dump(extract_hard_skills, f, indent=4) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skills Extraction Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKILLS = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HardSkills' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 45\u001b[0m\n\u001b[1;32m     39\u001b[0m         SKILLS[key] \u001b[38;5;241m=\u001b[39m justification\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(fixed_response) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkills were extracted successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;129m@tool\u001b[39m(parse_docstring\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodify_skills\u001b[39m(skill_name:\u001b[38;5;28mstr\u001b[39m, user_explanation:\u001b[38;5;28mstr\u001b[39m, action:\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m  \u001b[43mHardSkills\u001b[49m:\n\u001b[1;32m     46\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Responsible for Adding, Modifying or Deleting skills from skill set extracted from the resume, only when explicitly asked by the user.Takes in a skill_name , user explanation for the action to be done on the skill and action.Actions can be add, delete or modify. \u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m        action: add or delete\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# Invoke the LLM\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HardSkills' is not defined"
     ]
    }
   ],
   "source": [
    "# A tool consists of name of the tool, Description of the tool , A JSON schema defining the inputs to the tool and A function\n",
    "@tool(parse_docstring=True)\n",
    "def get_hard_skills(resume_text: str) ->  str:\n",
    "    \"\"\"Takes in User's Resume Text, extracts hard skills form resume , return a JSON of all extracted skills and their explanations using context from Resume.\n",
    "    \n",
    "    Args:\n",
    "        resume_text: Resume String for skill extraction.\n",
    "    \"\"\"\n",
    "    # Invoke the LLM\n",
    "    parser = PydanticOutputParser(pydantic_object= HardSkills)\n",
    "    \n",
    "    fix_parser = OutputFixingParser(\n",
    "        parser=parser,\n",
    "        retry_chain=fix_chain,\n",
    "        max_retries=2\n",
    "    )\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template = hard_skill_entity_extraction_prompt, \n",
    "        input_variables=[\"text\"],\n",
    "        partial_variables= {\"format_instructions\": parser.get_format_instructions()})\n",
    "    \n",
    "    prompt_str = prompt.format(text=resume_text)\n",
    "\n",
    "    print(prompt_str)\n",
    "    \n",
    "    response = llm.invoke(prompt_str)\n",
    "\n",
    "\n",
    "    print(f\"Response is : {response}\")\n",
    "\n",
    "    fixed_response = fix_parser.invoke(response).dict()\n",
    "\n",
    "    for skill_set in fixed_response[\"hard_skills\"]:\n",
    "        \n",
    "        key = skill_set[\"skill\"].lower()\n",
    "        justification = skill_set[\"skill_explanation\"]\n",
    "\n",
    "        SKILLS[key] = justification\n",
    "\n",
    "    return str(fixed_response) + \"Skills were extracted successfully!\"\n",
    "\n",
    "\n",
    "@tool(parse_docstring=True)\n",
    "def modify_skills(skill_name:str, user_explanation:str, action:str) ->  HardSkills:\n",
    "    \"\"\"Responsible for Adding, Modifying or Deleting skills from skill set extracted from the resume, only when explicitly asked by the user.Takes in a skill_name , user explanation for the action to be done on the skill and action.Actions can be add, delete or modify. \n",
    "    \n",
    "    Args:\n",
    "        skill_name: Name of the skill modification needs to be done on\n",
    "        user_explanation: Explanation of the modification\n",
    "        action: add or delete\n",
    "    \"\"\"\n",
    "    \n",
    "    # Invoke the LLM\n",
    "    parser = PydanticOutputParser(pydantic_object= HardSkill)\n",
    "    \n",
    "    fix_parser = OutputFixingParser(\n",
    "        parser=parser,\n",
    "        retry_chain=fix_chain,\n",
    "        max_retries=2\n",
    "    )\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template = modify_skills_entity_extraction_prompt, \n",
    "        input_variables=[\"skill_name\",\"user_justification\", \"all_skills\"],\n",
    "        partial_variables= {\"format_instructions\": parser.get_format_instructions()})\n",
    "    \n",
    "    prompt_str = prompt.format(skill_name=skill_name, user_justification=user_explanation, all_skills = SKILLS, action = action)\n",
    "\n",
    "    print(prompt_str)\n",
    "    \n",
    "    response = llm.invoke(prompt_str)\n",
    "\n",
    "\n",
    "    print(f\"Response is : {response}\")\n",
    "\n",
    "    fixed_response = fix_parser.invoke(response).dict()\n",
    "\n",
    "\n",
    "    if fixed_response[\"skill\"] and fixed_response[\"skill_explanation\"]:\n",
    "        key = fixed_response[\"skill\"].lower()\n",
    "        justification = fixed_response[\"skill_explanation\"]\n",
    "        if key not in SKILLS:\n",
    "            SKILLS[key] = justification\n",
    "            return f\"Skill: {skill_name} was Added Sucessfully.\"\n",
    "        else:\n",
    "            SKILLS[key] = justification\n",
    "            return f\"Skill: {skill_name} was Modified Sucessfully.\"\n",
    "    if fixed_response[\"skill\"] and not fixed_response[\"skill_explanation\"]:\n",
    "        key = fixed_response[\"skill\"].lower()\n",
    "        del SKILLS[key]\n",
    "        return f\"Skill: {skill_name} was Deleted Sucessfully\"\n",
    "    ß\n",
    "        \n",
    "\n",
    "    return \"No Action was performed, please elaborate further.\"\n",
    "\n",
    "get_hard_skills.args_schema.schema()\n",
    "modify_skills.args_schema.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "model = ChatBedrock(model_id =\"anthropic.claude-3-haiku-20240307-v1:0\", model_kwargs={\"temperature\": 0, \"max_tokens\":6000, \"top_p\":0.1,\"top_k\":50})\n",
    "tools = [get_hard_skills, modify_skills]\n",
    "agent_executor = create_react_agent(model, tools, checkpointer=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSHUMAAN TYAGI\n",
      "HP: +65 80169372      Email: e0866249@u.nus.edu\n",
      "EDUCATION\n",
      "National University of Singapore Aug 2021 - May 2025\n",
      "Major: Bachelor of Computing in Computer Science(Honours)\n",
      "Second Major: Data Science.\n",
      "Specialisation Track: Software Engineering and Artificial Intelligence.\n",
      "GPA: 4.31/5.\n",
      " \n",
      "TECHNICAL SKILLS\n",
      "Proficient languages: Python, Java ,SQL,  Javascript, HTML and CSS\n",
      "Proficient frameworks/libraries: Pytorch, Numpy, Django, PostgreSQL, React & React Native,Node.js, \n",
      "Sklearn, Firebase, Excel\n",
      "Knowledge of: MongoDB, LINUX/UNIX administration , R, Azure Open AI, GPT prompting, \n",
      "embeddings, Spacy\n",
      " \n",
      "WORK EXPERIENCE\n",
      "AI Developer, SAP, Singapore Jan 2023 - Present\n",
      "Integrating GPT powered by Azure Open AI endpoints in the current pipeline to perform parsing of \n",
      "PDF documents.\n",
      "Improving GPT response efficiency by 5 percent and data security.\n",
      "Utilized different prompting techniques such as RAG and chain of thought to minimize AI \n",
      "hallucinations.\n",
      "Competed regular bimonthly sprints and planned work product on JIRA.\n",
      "Learned more about SAP product's applications and implementation frameworks.\n",
      " \n",
      "Data Engineer Intern, Real Estate Analytics, Singapore, Singapore Aug 2023 - Present\n",
      "Extract, transform, and ingest real estate data from different sources and create workflows on Apache \n",
      "Airflow.\n",
      "Build a robust program for data extraction with various queries leveraging restful API and  GraphQL, \n",
      "and GUI scraping bots using selenium.\n",
      "Collaborated with data engineers in building efficient data pipelines using DBT, AWS etc.\n",
      " \n",
      "Full Stack Developer, ST Engineering, Singapore May 2023 - Jul 2023\n",
      "Designed Backend Architecture using UML Diagrams on Lucid Chart, involving extensible and intuitive \n",
      "design patterns suited to the application.\n",
      "Implemented OSI Network Layer influenced Serial Protocol using C# and .Net libraries.\n",
      "Connected Backend architecture with User interface with C# and WPF.\n",
      "Studied industrial application of SQL database use cases and it's deployment.\n",
      " \n",
      "Student Teaching Assistant, Programming Methodology I, II, National \n",
      "University of SingaporeAug 2022 - May 2023\n",
      "Nominated for Teacher's awards by 5/7 students and achieved a teacher's rating of 4.7/5, where \n",
      "faculty average is 4.5.\n",
      "Made NUS computing Honour List for Student Tutors in AY2022-2023 for excellence in teaching and \n",
      "contributions to learning at NUS.\n",
      " \n",
      "Software Engineer(Backend Developer), NUS CommIT, \n",
      "Singapore,SingaporeSep 2021 - May 2022\n",
      "Utilized Python, Django, Djoser etc. to design server applications and client interfaces.\n",
      "Handled development and management of frontend user interfaces with the help of HTML, CSS, and \n",
      "JavaScript.Collaborated with other developers to handle complicated issues related with deployment of Django \n",
      "based applications.\n",
      " \n",
      "PROJECTS\n",
      "Full Stack Engineer, Orbital (2022), Level Of Achievement : \n",
      "Artemis(Highest Level Possible)May 2022 - Aug 2022\n",
      "Designed, developed, tested and deployed mobile applications using React Native, Firebase and cloud \n",
      "firestore.\n",
      "Created a proposal with proper project development planning (Eg. Database Schema, UML Designs, \n",
      "etc) using Lucid Chart, swagger.io, etc.\n",
      "Implemented Google Firebase Cloud Firestore and Authentication to provide database and \n",
      "authentication services to application.\n",
      "Accomplished frontend design using Figma and executed using React Native, including pagination, \n",
      "routing using stack navigation, and live communication with firestore.\n",
      "Performed user testing , system integration testing and regression testing to test application \n",
      "functionality and cross-platform performance.\n"
     ]
    }
   ],
   "source": [
    "print(resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'usage': {'prompt_tokens': 1496, 'completion_tokens': 942, 'total_tokens': 2438}, 'stop_reason': 'tool_use', 'model_id': 'anthropic.claude-3-haiku-20240307-v1:0'}, response_metadata={'usage': {'prompt_tokens': 1496, 'completion_tokens': 942, 'total_tokens': 2438}, 'stop_reason': 'tool_use', 'model_id': 'anthropic.claude-3-haiku-20240307-v1:0'}, id='run-000bf4fd-7849-4de0-946c-46417ea099b1-0', tool_calls=[{'name': 'get_hard_skills', 'args': {'resume_text': \"ANSHUMAAN TYAGI\\nHP: +65 80169372      Email: e0866249@u.nus.edu\\nEDUCATION\\nNational University of Singapore Aug 2021 - May 2025\\nMajor: Bachelor of Computing in Computer Science(Honours)\\nSecond Major: Data Science.\\nSpecialisation Track: Software Engineering and Artificial Intelligence.\\nGPA: 4.31/5.\\n \\nTECHNICAL SKILLS\\nProficient languages: Python, Java ,SQL,  Javascript, HTML and CSS\\nProficient frameworks/libraries: Pytorch, Numpy, Django, PostgreSQL, React & React Native,Node.js, \\nSklearn, Firebase, Excel\\nKnowledge of: MongoDB, LINUX/UNIX administration , R, Azure Open AI, GPT prompting, \\nembeddings, Spacy\\n \\nWORK EXPERIENCE\\nAI Developer, SAP, Singapore Jan 2023 - Present\\nIntegrating GPT powered by Azure Open AI endpoints in the current pipeline to perform parsing of \\nPDF documents.\\nImproving GPT response efficiency by 5 percent and data security.\\nUtilized different prompting techniques such as RAG and chain of thought to minimize AI \\nhallucinations.\\nCompeted regular bimonthly sprints and planned work product on JIRA.\\nLearned more about SAP product's applications and implementation frameworks.\\n \\nData Engineer Intern, Real Estate Analytics, Singapore, Singapore Aug 2023 - Present\\nExtract, transform, and ingest real estate data from different sources and create workflows on Apache \\nAirflow.\\nBuild a robust program for data extraction with various queries leveraging restful API and  GraphQL, \\nand GUI scraping bots using selenium.\\nCollaborated with data engineers in building efficient data pipelines using DBT, AWS etc.\\n \\nFull Stack Developer, ST Engineering, Singapore May 2023 - Jul 2023\\nDesigned Backend Architecture using UML Diagrams on Lucid Chart, involving extensible and intuitive \\ndesign patterns suited to the application.\\nImplemented OSI Network Layer influenced Serial Protocol using C# and .Net libraries.\\nConnected Backend architecture with User interface with C# and WPF.\\nStudied industrial application of SQL database use cases and it's deployment.\\n \\nStudent Teaching Assistant, Programming Methodology I, II, National \\nUniversity of SingaporeAug 2022 - May 2023\\nNominated for Teacher's awards by 5/7 students and achieved a teacher's rating of 4.7/5, where \\nfaculty average is 4.5.\\nMade NUS computing Honour List for Student Tutors in AY2022-2023 for excellence in teaching and \\ncontributions to learning at NUS.\\n \\nSoftware Engineer(Backend Developer), NUS CommIT, \\nSingapore,SingaporeSep 2021 - May 2022\\nUtilized Python, Django, Djoser etc. to design server applications and client interfaces.\\nHandled development and management of frontend user interfaces with the help of HTML, CSS, and \\nJavaScript.Collaborated with other developers to handle complicated issues related with deployment of Django \\nbased applications.\\n \\nPROJECTS\\nFull Stack Engineer, Orbital (2022), Level Of Achievement : \\nArtemis(Highest Level Possible)May 2022 - Aug 2022\\nDesigned, developed, tested and deployed mobile applications using React Native, Firebase and cloud \\nfirestore.\\nCreated a proposal with proper project development planning (Eg. Database Schema, UML Designs, \\netc) using Lucid Chart, swagger.io, etc.\\nImplemented Google Firebase Cloud Firestore and Authentication to provide database and \\nauthentication services to application.\\nAccomplished frontend design using Figma and executed using React Native, including pagination, \\nrouting using stack navigation, and live communication with firestore.\\nPerformed user testing , system integration testing and regression testing to test application \\nfunctionality and cross-platform performance.\"}, 'id': 'toolu_bdrk_012JZdZJbAPk7CayLtTqj5YE', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1496, 'output_tokens': 942, 'total_tokens': 2438})]}}\n",
      "----\n",
      "\n",
      "You are a career counselor. Your task is to extract skill entities from the given text, which can be a resume or a job description.\n",
      "\n",
      "Skill Entities:\n",
      "Hard Skills: Also called technical skills, these are job-specific and relevant to each position and seniority level. In other words, each position in every company will require unique hard skills.\n",
      "\n",
      "TASK:\n",
      "1. Perform a Part-of-Speech (POS) Tagging on the text.\n",
      "2. Using the POS-tagged resume, perform Name Entity Recognition to identify all explicitly stated skill entities.\n",
      "3. For each skill, provide an explanation of skill demonstration using context from the text and the context's contribution to the skill.\n",
      "4. State the context as it appears in the resume, without extrapolating it with other experiences.\n",
      "\n",
      "----------------------\n",
      "Format Instruction:\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"$defs\": {\"HardSkill\": {\"properties\": {\"skill\": {\"description\": \"Name of the skill\", \"title\": \"Skill\", \"type\": \"string\"}, \"skill_explanation\": {\"description\": \"explanation of skill demonstration using context from the text and the context's contribution to the skill\", \"title\": \"Skill Explanation\", \"type\": \"string\"}}, \"required\": [\"skill\", \"skill_explanation\"], \"title\": \"HardSkill\", \"type\": \"object\"}}, \"properties\": {\"hard_skills\": {\"description\": \"List of hard skills \", \"items\": {\"$ref\": \"#/$defs/HardSkill\"}, \"title\": \"Hard Skills\", \"type\": \"array\"}}, \"required\": [\"hard_skills\"]}\n",
      "```\n",
      "----------------------\n",
      "\n",
      "----------------------\n",
      "Text: ANSHUMAAN TYAGI\n",
      "HP: +65 80169372      Email: e0866249@u.nus.edu\n",
      "EDUCATION\n",
      "National University of Singapore Aug 2021 - May 2025\n",
      "Major: Bachelor of Computing in Computer Science(Honours)\n",
      "Second Major: Data Science.\n",
      "Specialisation Track: Software Engineering and Artificial Intelligence.\n",
      "GPA: 4.31/5.\n",
      " \n",
      "TECHNICAL SKILLS\n",
      "Proficient languages: Python, Java ,SQL,  Javascript, HTML and CSS\n",
      "Proficient frameworks/libraries: Pytorch, Numpy, Django, PostgreSQL, React & React Native,Node.js, \n",
      "Sklearn, Firebase, Excel\n",
      "Knowledge of: MongoDB, LINUX/UNIX administration , R, Azure Open AI, GPT prompting, \n",
      "embeddings, Spacy\n",
      " \n",
      "WORK EXPERIENCE\n",
      "AI Developer, SAP, Singapore Jan 2023 - Present\n",
      "Integrating GPT powered by Azure Open AI endpoints in the current pipeline to perform parsing of \n",
      "PDF documents.\n",
      "Improving GPT response efficiency by 5 percent and data security.\n",
      "Utilized different prompting techniques such as RAG and chain of thought to minimize AI \n",
      "hallucinations.\n",
      "Competed regular bimonthly sprints and planned work product on JIRA.\n",
      "Learned more about SAP product's applications and implementation frameworks.\n",
      " \n",
      "Data Engineer Intern, Real Estate Analytics, Singapore, Singapore Aug 2023 - Present\n",
      "Extract, transform, and ingest real estate data from different sources and create workflows on Apache \n",
      "Airflow.\n",
      "Build a robust program for data extraction with various queries leveraging restful API and  GraphQL, \n",
      "and GUI scraping bots using selenium.\n",
      "Collaborated with data engineers in building efficient data pipelines using DBT, AWS etc.\n",
      " \n",
      "Full Stack Developer, ST Engineering, Singapore May 2023 - Jul 2023\n",
      "Designed Backend Architecture using UML Diagrams on Lucid Chart, involving extensible and intuitive \n",
      "design patterns suited to the application.\n",
      "Implemented OSI Network Layer influenced Serial Protocol using C# and .Net libraries.\n",
      "Connected Backend architecture with User interface with C# and WPF.\n",
      "Studied industrial application of SQL database use cases and it's deployment.\n",
      " \n",
      "Student Teaching Assistant, Programming Methodology I, II, National \n",
      "University of SingaporeAug 2022 - May 2023\n",
      "Nominated for Teacher's awards by 5/7 students and achieved a teacher's rating of 4.7/5, where \n",
      "faculty average is 4.5.\n",
      "Made NUS computing Honour List for Student Tutors in AY2022-2023 for excellence in teaching and \n",
      "contributions to learning at NUS.\n",
      " \n",
      "Software Engineer(Backend Developer), NUS CommIT, \n",
      "Singapore,SingaporeSep 2021 - May 2022\n",
      "Utilized Python, Django, Djoser etc. to design server applications and client interfaces.\n",
      "Handled development and management of frontend user interfaces with the help of HTML, CSS, and \n",
      "JavaScript.Collaborated with other developers to handle complicated issues related with deployment of Django \n",
      "based applications.\n",
      " \n",
      "PROJECTS\n",
      "Full Stack Engineer, Orbital (2022), Level Of Achievement : \n",
      "Artemis(Highest Level Possible)May 2022 - Aug 2022\n",
      "Designed, developed, tested and deployed mobile applications using React Native, Firebase and cloud \n",
      "firestore.\n",
      "Created a proposal with proper project development planning (Eg. Database Schema, UML Designs, \n",
      "etc) using Lucid Chart, swagger.io, etc.\n",
      "Implemented Google Firebase Cloud Firestore and Authentication to provide database and \n",
      "authentication services to application.\n",
      "Accomplished frontend design using Figma and executed using React Native, including pagination, \n",
      "routing using stack navigation, and live communication with firestore.\n",
      "Performed user testing , system integration testing and regression testing to test application \n",
      "functionality and cross-platform performance.\n",
      "----------------------\n",
      "\n",
      "Response is : \n",
      "\n",
      "```json\n",
      "{\n",
      "  \"hard_skills\": [\n",
      "    {\n",
      "      \"skill\": \"Python\",\n",
      "      \"skill_explanation\": \"Used Python to design server applications and client interfaces in the role of Software Engineer(Backend Developer) at NUS CommIT.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"Java\",\n",
      "      \"skill_explanation\": \"Not explicitly demonstrated in the text.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"SQL\",\n",
      "      \"skill_explanation\": \"Studied industrial application of SQL database use cases and it's deployment in the role of Full Stack Developer at ST Engineering. Implemented OSI Network Layer influenced Serial Protocol using C# and .Net libraries.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"Javascript\",\n",
      "      \"skill_explanation\": \"Handled development and management of frontend user interfaces with the help of HTML, CSS, and Javascript in the role of Software Engineer(Backend Developer) at NUS CommIT.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"HTML\",\n",
      "      \"skill_explanation\": \"Handled development and management of frontend user interfaces with the help of HTML, CSS, and Javascript in the role of Software Engineer(Backend Developer) at NUS CommIT.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"CSS\",\n",
      "      \"skill_explanation\": \"Handled development and management of frontend user interfaces with the help of HTML, CSS, and Javascript in the role of Software Engineer(Backend Developer) at NUS CommIT.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"Pytorch\",\n",
      "      \"skill_explanation\": \"Integrating GPT powered by Azure Open AI endpoints in the current pipeline to perform parsing of PDF documents in the role of AI Developer at SAP.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"Numpy\",\n",
      "      \"skill_explanation\": \"Not explicitly demonstrated in the text.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"Django\",\n",
      "      \"skill_explanation\": \"Utilized Python, Django, Djoser etc. to design server applications and client interfaces in the role of Software Engineer(Backend Developer) at NUS CommIT.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"PostgreSQL\",\n",
      "      \"skill_explanation\": \"Not explicitly demonstrated in the text.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"React & React Native\",\n",
      "      \"skill_explanation\": \"Designed, developed, tested and deployed mobile applications using React Native, Firebase and cloud firestore in the role of Full Stack Engineer at Orbital.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"Node.js\",\n",
      "      \"skill_explanation\": \"Not explicitly demonstrated in the text.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"Sklearn\",\n",
      "      \"skill_explanation\": \"Not explicitly demonstrated in the text.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"Firebase\",\n",
      "      \"skill_explanation\": \"Implemented Google Firebase Cloud Firestore and Authentication to provide database and authentication services to application in the role of Full Stack Engineer at Orbital.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"Excel\",\n",
      "      \"skill_explanation\": \"Not explicitly demonstrated in the text.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"MongoDB\",\n",
      "      \"skill_explanation\": \"Not explicitly demonstrated in the text.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"LINUX/UNIX administration\",\n",
      "      \"skill_explanation\": \"Not explicitly demonstrated in the text.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"R\",\n",
      "      \"skill_explanation\": \"Not explicitly demonstrated in the text.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"Azure Open AI\",\n",
      "      \"skill_explanation\": \"Integrating GPT powered by Azure Open AI endpoints in the current pipeline to perform parsing of PDF documents in the role of AI Developer at SAP.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"GPT prompting\",\n",
      "      \"skill_explanation\": \"Utilized different prompting techniques such as RAG and chain of thought to minimize AI hallucinations in the role of AI Developer at SAP.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"embeddings\",\n",
      "      \"skill_explanation\": \"Not explicitly demonstrated in the text.\"\n",
      "    },\n",
      "    {\n",
      "      \"skill\": \"Spacy\",\n",
      "      \"skill_explanation\": \"Not explicitly demonstrated in the text.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "{'tools': {'messages': [ToolMessage(content='{\\'hard_skills\\': [{\\'skill\\': \\'Python\\', \\'skill_explanation\\': \\'Used Python to design server applications and client interfaces in the role of Software Engineer(Backend Developer) at NUS CommIT.\\'}, {\\'skill\\': \\'Java\\', \\'skill_explanation\\': \\'Not explicitly demonstrated in the text.\\'}, {\\'skill\\': \\'SQL\\', \\'skill_explanation\\': \"Studied industrial application of SQL database use cases and it\\'s deployment in the role of Full Stack Developer at ST Engineering. Implemented OSI Network Layer influenced Serial Protocol using C# and .Net libraries.\"}, {\\'skill\\': \\'Javascript\\', \\'skill_explanation\\': \\'Handled development and management of frontend user interfaces with the help of HTML, CSS, and Javascript in the role of Software Engineer(Backend Developer) at NUS CommIT.\\'}, {\\'skill\\': \\'HTML\\', \\'skill_explanation\\': \\'Handled development and management of frontend user interfaces with the help of HTML, CSS, and Javascript in the role of Software Engineer(Backend Developer) at NUS CommIT.\\'}, {\\'skill\\': \\'CSS\\', \\'skill_explanation\\': \\'Handled development and management of frontend user interfaces with the help of HTML, CSS, and Javascript in the role of Software Engineer(Backend Developer) at NUS CommIT.\\'}, {\\'skill\\': \\'Pytorch\\', \\'skill_explanation\\': \\'Integrating GPT powered by Azure Open AI endpoints in the current pipeline to perform parsing of PDF documents in the role of AI Developer at SAP.\\'}, {\\'skill\\': \\'Numpy\\', \\'skill_explanation\\': \\'Not explicitly demonstrated in the text.\\'}, {\\'skill\\': \\'Django\\', \\'skill_explanation\\': \\'Utilized Python, Django, Djoser etc. to design server applications and client interfaces in the role of Software Engineer(Backend Developer) at NUS CommIT.\\'}, {\\'skill\\': \\'PostgreSQL\\', \\'skill_explanation\\': \\'Not explicitly demonstrated in the text.\\'}, {\\'skill\\': \\'React & React Native\\', \\'skill_explanation\\': \\'Designed, developed, tested and deployed mobile applications using React Native, Firebase and cloud firestore in the role of Full Stack Engineer at Orbital.\\'}, {\\'skill\\': \\'Node.js\\', \\'skill_explanation\\': \\'Not explicitly demonstrated in the text.\\'}, {\\'skill\\': \\'Sklearn\\', \\'skill_explanation\\': \\'Not explicitly demonstrated in the text.\\'}, {\\'skill\\': \\'Firebase\\', \\'skill_explanation\\': \\'Implemented Google Firebase Cloud Firestore and Authentication to provide database and authentication services to application in the role of Full Stack Engineer at Orbital.\\'}, {\\'skill\\': \\'Excel\\', \\'skill_explanation\\': \\'Not explicitly demonstrated in the text.\\'}, {\\'skill\\': \\'MongoDB\\', \\'skill_explanation\\': \\'Not explicitly demonstrated in the text.\\'}, {\\'skill\\': \\'LINUX/UNIX administration\\', \\'skill_explanation\\': \\'Not explicitly demonstrated in the text.\\'}, {\\'skill\\': \\'R\\', \\'skill_explanation\\': \\'Not explicitly demonstrated in the text.\\'}, {\\'skill\\': \\'Azure Open AI\\', \\'skill_explanation\\': \\'Integrating GPT powered by Azure Open AI endpoints in the current pipeline to perform parsing of PDF documents in the role of AI Developer at SAP.\\'}, {\\'skill\\': \\'GPT prompting\\', \\'skill_explanation\\': \\'Utilized different prompting techniques such as RAG and chain of thought to minimize AI hallucinations in the role of AI Developer at SAP.\\'}, {\\'skill\\': \\'embeddings\\', \\'skill_explanation\\': \\'Not explicitly demonstrated in the text.\\'}, {\\'skill\\': \\'Spacy\\', \\'skill_explanation\\': \\'Not explicitly demonstrated in the text.\\'}]}Skills were extracted successfully!', name='get_hard_skills', tool_call_id='toolu_bdrk_012JZdZJbAPk7CayLtTqj5YE')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content='The key hard skills extracted from the resume are:\\n- Python\\n- SQL \\n- Javascript\\n- HTML\\n- CSS\\n- Pytorch\\n- Django\\n- React & React Native\\n- Firebase\\n- Azure Open AI\\n- GPT prompting\\n\\nThe resume demonstrates proficiency in these technical skills through the various work experiences and projects described.', additional_kwargs={'usage': {'prompt_tokens': 3268, 'completion_tokens': 76, 'total_tokens': 3344}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-haiku-20240307-v1:0'}, response_metadata={'usage': {'prompt_tokens': 3268, 'completion_tokens': 76, 'total_tokens': 3344}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-haiku-20240307-v1:0'}, id='run-ce3ea051-16b4-4d3c-8325-87b69f53e48f-0', usage_metadata={'input_tokens': 3268, 'output_tokens': 76, 'total_tokens': 3344})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "q1 = f\"Hey, Extract my skills, my resume is: {resume_text}\"\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "\n",
    "for chunk in agent_executor.stream(\n",
    "{\"messages\": [HumanMessage(content=q1)]}, config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"----\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'python': 'Used Python to design server applications and client interfaces in the role of Software Engineer(Backend Developer) at NUS CommIT.', 'java': 'Not explicitly demonstrated in the text.', 'sql': \"Studied industrial application of SQL database use cases and it's deployment in the role of Full Stack Developer at ST Engineering. Utilized SQL database in the role of Software Engineer(Backend Developer) at NUS CommIT.\", 'javascript': 'Handled development and management of frontend user interfaces with the help of HTML, CSS, and Javascript in the role of Software Engineer(Backend Developer) at NUS CommIT.', 'html': 'Handled development and management of frontend user interfaces with the help of HTML, CSS, and Javascript in the role of Software Engineer(Backend Developer) at NUS CommIT.', 'css': 'Handled development and management of frontend user interfaces with the help of HTML, CSS, and Javascript in the role of Software Engineer(Backend Developer) at NUS CommIT.', 'pytorch': 'Not explicitly demonstrated in the text.', 'numpy': 'Not explicitly demonstrated in the text.', 'django': 'Utilized Python, Django, Djoser etc. to design server applications and client interfaces in the role of Software Engineer(Backend Developer) at NUS CommIT.', 'postgresql': 'Not explicitly demonstrated in the text.', 'react & react native': 'Designed, developed, tested and deployed mobile applications using React Native, Firebase and cloud firestore in the role of Full Stack Engineer at Orbital.', 'node.js': 'Not explicitly demonstrated in the text.', 'sklearn': 'Not explicitly demonstrated in the text.', 'firebase': 'Implemented Google Firebase Cloud Firestore and Authentication to provide database and authentication services to application in the role of Full Stack Engineer at Orbital.', 'excel': 'Not explicitly demonstrated in the text.', 'mongodb': 'Not explicitly demonstrated in the text.', 'linux/unix administration': 'Not explicitly demonstrated in the text.', 'r': 'Not explicitly demonstrated in the text.', 'azure open ai': 'Integrating GPT powered by Azure Open AI endpoints in the current pipeline to perform parsing of PDF documents in the role of AI Developer at SAP.', 'gpt prompting, embeddings, spacy': 'Utilized different prompting techniques such as RAG and chain of thought to minimize AI hallucinations in the role of AI Developer at SAP.', 'apache airflow': 'Extract, transform, and ingest real estate data from different sources and create workflows on Apache Airflow in the role of Data Engineer Intern at Real Estate Analytics.', 'dbt, aws': 'Collaborated with data engineers in building efficient data pipelines using DBT, AWS etc. in the role of Data Engineer Intern at Real Estate Analytics.', 'c#, .net libraries': 'Implemented OSI Network Layer influenced Serial Protocol using C# and .Net libraries in the role of Full Stack Developer at ST Engineering.', 'lucid chart, uml diagrams': 'Designed Backend Architecture using UML Diagrams on Lucid Chart in the role of Full Stack Developer at ST Engineering.', 'wpf': 'Connected Backend architecture with User interface with C# and WPF in the role of Full Stack Developer at ST Engineering.'}\n"
     ]
    }
   ],
   "source": [
    "print(SKILLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'usage': {'prompt_tokens': 3360, 'completion_tokens': 144, 'total_tokens': 3504}, 'stop_reason': 'tool_use', 'model_id': 'anthropic.claude-3-haiku-20240307-v1:0'}, response_metadata={'usage': {'prompt_tokens': 3360, 'completion_tokens': 144, 'total_tokens': 3504}, 'stop_reason': 'tool_use', 'model_id': 'anthropic.claude-3-haiku-20240307-v1:0'}, id='run-73c02dc9-8d50-4430-9435-4c3c76295f07-0', tool_calls=[{'name': 'modify_skills', 'args': {'skill_name': 'GPT prompting', 'user_explanation': 'The resume mentions that I utilized different prompting techniques such as RAG and chain of thought to minimize AI hallucinations in my role as an AI Developer at SAP. This shows that I have experience working with and fine-tuning GPT models.', 'action': 'modify'}, 'id': 'toolu_bdrk_014vtZ58sFuzZrkLwujGkARw', 'type': 'tool_call'}], usage_metadata={'input_tokens': 3360, 'output_tokens': 144, 'total_tokens': 3504})]}}\n",
      "----\n",
      "\n",
      "\n",
      "TASK: \n",
      "1. Understand User Modified Skill Name,Modification justification and the modification action user wants to perform.\n",
      "2. For add or modify Action , check if the skill_name already exists, if it does use the explanation to add to the current skill description.\n",
      "2. For delete action, use the explanation to determine if the whole skills needs to be deleted or just parts of the current justification.\n",
      "\n",
      "Only return the affected skill_name and skill_justification. \n",
      "If the whole skill is deleted , return an empty justification.\n",
      "If the user input justification is not enough or enough infromation about the action is not provided, simply return an empty skill_name.\n",
      "\n",
      "Only return the JSON. \n",
      "------------------------------- \n",
      "Format Instruction: \n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"skill\": {\"description\": \"Name of the skill\", \"title\": \"Skill\", \"type\": \"string\"}, \"skill_explanation\": {\"description\": \"explanation of skill demonstration using context from the text and the context's contribution to the skill\", \"title\": \"Skill Explanation\", \"type\": \"string\"}}, \"required\": [\"skill\", \"skill_explanation\"]}\n",
      "```\n",
      "------------------------------- \n",
      "\n",
      "------------------------------- \n",
      "user input skill: GPT prompting\n",
      "user justification: The resume mentions that I utilized different prompting techniques such as RAG and chain of thought to minimize AI hallucinations in my role as an AI Developer at SAP. This shows that I have experience working with and fine-tuning GPT models.\n",
      "action: modify\n",
      "All Skills: {'python': 'Used Python to design server applications and client interfaces in the role of Software Engineer(Backend Developer) at NUS CommIT.', 'java': 'Not explicitly demonstrated in the text.', 'sql': \"Studied industrial application of SQL database use cases and it's deployment in the role of Full Stack Developer at ST Engineering. Implemented OSI Network Layer influenced Serial Protocol using C# and .Net libraries.\", 'javascript': 'Handled development and management of frontend user interfaces with the help of HTML, CSS, and Javascript in the role of Software Engineer(Backend Developer) at NUS CommIT.', 'html': 'Handled development and management of frontend user interfaces with the help of HTML, CSS, and Javascript in the role of Software Engineer(Backend Developer) at NUS CommIT.', 'css': 'Handled development and management of frontend user interfaces with the help of HTML, CSS, and Javascript in the role of Software Engineer(Backend Developer) at NUS CommIT.', 'pytorch': 'Integrating GPT powered by Azure Open AI endpoints in the current pipeline to perform parsing of PDF documents in the role of AI Developer at SAP.', 'numpy': 'Not explicitly demonstrated in the text.', 'django': 'Utilized Python, Django, Djoser etc. to design server applications and client interfaces in the role of Software Engineer(Backend Developer) at NUS CommIT.', 'postgresql': 'Not explicitly demonstrated in the text.', 'react & react native': 'Designed, developed, tested and deployed mobile applications using React Native, Firebase and cloud firestore in the role of Full Stack Engineer at Orbital.', 'node.js': 'Not explicitly demonstrated in the text.', 'sklearn': 'Not explicitly demonstrated in the text.', 'firebase': 'Implemented Google Firebase Cloud Firestore and Authentication to provide database and authentication services to application in the role of Full Stack Engineer at Orbital.', 'excel': 'Not explicitly demonstrated in the text.', 'mongodb': 'Not explicitly demonstrated in the text.', 'linux/unix administration': 'Not explicitly demonstrated in the text.', 'r': 'Not explicitly demonstrated in the text.', 'azure open ai': 'Integrating GPT powered by Azure Open AI endpoints in the current pipeline to perform parsing of PDF documents in the role of AI Developer at SAP.', 'gpt prompting, embeddings, spacy': 'Utilized different prompting techniques such as RAG and chain of thought to minimize AI hallucinations in the role of AI Developer at SAP.', 'apache airflow': 'Extract, transform, and ingest real estate data from different sources and create workflows on Apache Airflow in the role of Data Engineer Intern at Real Estate Analytics.', 'dbt, aws': 'Collaborated with data engineers in building efficient data pipelines using DBT, AWS etc. in the role of Data Engineer Intern at Real Estate Analytics.', 'c#, .net libraries': 'Implemented OSI Network Layer influenced Serial Protocol using C# and .Net libraries in the role of Full Stack Developer at ST Engineering.', 'lucid chart, uml diagrams': 'Designed Backend Architecture using UML Diagrams on Lucid Chart in the role of Full Stack Developer at ST Engineering.', 'wpf': 'Connected Backend architecture with User interface with C# and WPF in the role of Full Stack Developer at ST Engineering.', 'gpt prompting': 'Utilized different prompting techniques such as RAG and chain of thought to minimize AI hallucinations in the role of AI Developer at SAP.', 'embeddings': 'Not explicitly demonstrated in the text.', 'spacy': 'Not explicitly demonstrated in the text.'}\n",
      "-------------------------------\n",
      "\n",
      "Response is : \n",
      "\n",
      "{\n",
      "  \"skill\": \"GPT prompting\",\n",
      "  \"skill_explanation\": \"The resume mentions that I utilized different prompting techniques such as RAG and chain of thought to minimize AI hallucinations in my role as an AI Developer at SAP. This shows that I have experience working with and fine-tuning GPT models.\"\n",
      "}\n",
      "{'tools': {'messages': [ToolMessage(content='Skill: GPT prompting was Modified Sucessfully.', name='modify_skills', tool_call_id='toolu_bdrk_014vtZ58sFuzZrkLwujGkARw')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content='The resume indicates that in your role as an AI Developer at SAP, you utilized different prompting techniques such as RAG (Retrieval Augmented Generation) and chain of thought to minimize AI hallucinations when integrating GPT models powered by Azure Open AI. This demonstrates your experience in fine-tuning and optimizing the performance of large language models like GPT for specific applications.', additional_kwargs={'usage': {'prompt_tokens': 3528, 'completion_tokens': 85, 'total_tokens': 3613}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-haiku-20240307-v1:0'}, response_metadata={'usage': {'prompt_tokens': 3528, 'completion_tokens': 85, 'total_tokens': 3613}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-haiku-20240307-v1:0'}, id='run-f5eb1a07-8cc5-4501-a70f-51d2b4163138-0', usage_metadata={'input_tokens': 3528, 'output_tokens': 85, 'total_tokens': 3613})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "q2 = \"Can you elt me know how I used GPT models?\"\n",
    "\n",
    "for chunk in agent_executor.stream(\n",
    "{\"messages\": [HumanMessage(content=q2)]}, config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"----\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python': 'Used Python to design server applications and client interfaces in the role of Software Engineer(Backend Developer) at NUS CommIT.',\n",
       " 'java': 'Not explicitly demonstrated in the text.',\n",
       " 'sql': \"Studied industrial application of SQL database use cases and it's deployment in the role of Full Stack Developer at ST Engineering. Utilized SQL database in the role of Software Engineer(Backend Developer) at NUS CommIT.\",\n",
       " 'javascript': 'Handled development and management of frontend user interfaces with the help of HTML, CSS, and Javascript in the role of Software Engineer(Backend Developer) at NUS CommIT.',\n",
       " 'html': 'Handled development and management of frontend user interfaces with the help of HTML, CSS, and Javascript in the role of Software Engineer(Backend Developer) at NUS CommIT.',\n",
       " 'css': 'Handled development and management of frontend user interfaces with the help of HTML, CSS, and Javascript in the role of Software Engineer(Backend Developer) at NUS CommIT.',\n",
       " 'pytorch': 'Not explicitly demonstrated in the text.',\n",
       " 'numpy': 'Not explicitly demonstrated in the text.',\n",
       " 'django': 'Utilized Python, Django, Djoser etc. to design server applications and client interfaces in the role of Software Engineer(Backend Developer) at NUS CommIT.',\n",
       " 'postgresql': 'Not explicitly demonstrated in the text.',\n",
       " 'react & react native': 'Designed, developed, tested and deployed mobile applications using React Native, Firebase and cloud firestore in the role of Full Stack Engineer at Orbital.',\n",
       " 'node.js': 'Not explicitly demonstrated in the text.',\n",
       " 'sklearn': 'Not explicitly demonstrated in the text.',\n",
       " 'firebase': 'Implemented Google Firebase Cloud Firestore and Authentication to provide database and authentication services to application in the role of Full Stack Engineer at Orbital.',\n",
       " 'excel': 'Not explicitly demonstrated in the text.',\n",
       " 'mongodb': 'Not explicitly demonstrated in the text.',\n",
       " 'linux/unix administration': 'Not explicitly demonstrated in the text.',\n",
       " 'r': 'Not explicitly demonstrated in the text.',\n",
       " 'azure open ai': 'Integrating GPT powered by Azure Open AI endpoints in the current pipeline to perform parsing of PDF documents in the role of AI Developer at SAP.',\n",
       " 'gpt prompting, embeddings, spacy': 'Utilized different prompting techniques such as RAG and chain of thought to minimize AI hallucinations in the role of AI Developer at SAP.',\n",
       " 'apache airflow': 'Extract, transform, and ingest real estate data from different sources and create workflows on Apache Airflow in the role of Data Engineer Intern at Real Estate Analytics.',\n",
       " 'dbt, aws': 'Collaborated with data engineers in building efficient data pipelines using DBT, AWS etc. in the role of Data Engineer Intern at Real Estate Analytics.',\n",
       " 'c#, .net libraries': 'Implemented OSI Network Layer influenced Serial Protocol using C# and .Net libraries in the role of Full Stack Developer at ST Engineering.',\n",
       " 'lucid chart, uml diagrams': 'Designed Backend Architecture using UML Diagrams on Lucid Chart in the role of Full Stack Developer at ST Engineering.',\n",
       " 'wpf': 'Connected Backend architecture with User interface with C# and WPF in the role of Full Stack Developer at ST Engineering.'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SKILLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the extracted competencies to a JSON file\n",
    "\n",
    "import json\n",
    "\n",
    "with open(f\"candidate_skills_extracted.json\", \"w\") as f:\n",
    "    json.dump(SKILLS, f, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 6: Streamlit Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.callbacks.streamlit import (\n",
    "    StreamlitCallbackHandler,\n",
    ")\n",
    "import streamlit as st\n",
    "\n",
    "st_callback = StreamlitCallbackHandler(st.container())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 22:09:34.772 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/anshumaantyagi/Desktop/CourseRec/env/lib/python3.9/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "if prompt := st.chat_input():\n",
    "    st.chat_message(\"user\").write(prompt)\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        st_callback = StreamlitCallbackHandler(st.container())\n",
    "        response = agent_executor.invoke(\n",
    "            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n",
    "        )\n",
    "        st.write(response[\"output\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
